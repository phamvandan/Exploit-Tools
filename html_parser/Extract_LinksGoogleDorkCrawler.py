import requests
import urllib
import pandas as pd
from requests_html import HTML
from requests_html import HTMLSession
import sys 
from urllib.parse import urlparse

def get_source(url):
    """Return the source code for the provided URL. 

    Args: 
        url (string): URL of the page to scrape.

    Returns:
        response (object): HTTP response object from requests_html. 
    """

    try:
        session = HTMLSession()
        response = session.get(url)
        return response

    except requests.exceptions.RequestException as e:
        print(e)

def scrape_google(query, page=1):

    query = urllib.parse.quote_plus(query)
    response = get_source("https://www.google.com/search?q=" + query + '&start=' + str(page*10))
    # response = get_source(url)

    links = list(response.html.absolute_links)
    google_domains = ('https://www.google.', 
                      'https://google.', 
                      'https://webcache.googleusercontent.', 
                      'http://webcache.googleusercontent.', 
                      'https://policies.google.',
                      'https://support.google.',
                      'https://maps.google.',
                      'https://translate.google.com')

    for url in links[:]:
        if url.startswith(google_domains):
            links.remove(url)

    return links

f = open(sys.argv[1])
queries = f.readlines()
links = []
domains = []
page = 3
for query in queries:
    query = query.replace('\n', '')
    for i in range(page):
        links.extend(scrape_google(query, page=i+1))
f.close()
links = list(dict.fromkeys(links))
for link in links:
    print('Domain:', urlparse('http://www.example.test/foo/bar').netloc)
pd.DataFrame(links, columns=['link']).to_csv('link.csv', index=False)